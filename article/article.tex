\documentclass[10pt, a4paper,spanish]{article}

\usepackage{mystyle}
\usepackage{myvars}



%-----------------------------

\begin{document}

	\maketitle % Insert title

	\thispagestyle{fancy} % All pages have headers and footers


%-----------------------------
%	ABSTRACT
%-----------------------------

	\begin{abstract}
		\noindent En este documento se analiza el comportamiento de los algoritmos de generación de árboles de decisión \emph{ID3} y \emph{J48} desde el punto de vista de la discretización de atributos (en el caso de \emph{ID3} de forma manual). Para ello se ha utilizado el conjunto de datos \emph{Thoracic Surgery Data Data Set}\cite{dataset:thoracic}. La herramienta utilizada para el aprendizaje automático ha sido \emph{WEKA}\cite{tool:weka}. La metodología experimental que se ha seguido a lo largo del documento ha sido un \emph{Hold-Out} de $\frac{1}{3}$ para tareas de prueba.
	\end{abstract}

%-----------------------------
%	TEXT
%-----------------------------


	\section{¿Por qué no se puede aplicar directamente ID3?}
	\label{sec:1}

		\paragraph{}
		El conjunto de datos utilizado\cite{dataset:thoracic} está formado por \textbf{470} instancias, las cuales describen resultados acerca de la esperanza de vida tras operaciones de cáncer pulmonar. Dicho conjunto presenta atributos con las siguientes características:
		\begin{itemize}
			\setlength\itemsep{0em}
			\item 10 atributos de tipo categórico binario.
			\item 1 atributo de tipo categórico con 3 valores distintos.
			\item 1 atributo de tipo categórico con 4 valores distintos.
			\item 1 atributo de tipo categórico con 7 valores distintos.
			\item 1 atributo de tipo numérico real en el rango $[1.44, 6.3]$.
			\item 1 atributo de tipo numérico real en el rango $[0.96, 86.3]$.
			\item 1 atributo de tipo numérico entero en el rango $[21, 87]$.
			\item 1 atributo de tipo categórico binario, que representa el valor de la clase.
		\end{itemize}

		\paragraph{}
		La razón por la cual no se puede aplicar el algoritmo \emph{ID3} sobre el mismo es la existencia de atributos de naturaleza numérica, para los cuales dicho algoritmo no está diseñado. Por lo tanto, para que la clasificación con este algoritmo pueda llevarse a cabo, será necesaria una tarea de adaptación previa del conjunto de datos a los requisito de \emph{ID3}

	\section{Realice las modificaciones previas en el fichero de datos para que pueda llevar a cabo lo anterior y proporcione los resultados aplicando el método de retención o Hold-Out para la formación del experimento}
	\label{sec:2}

		\paragraph{}
		Para poder aplicar el algoritmo \emph{ID3} existen distintas alternativas, entre ellas se encuentran la discretización de los atributos numéricos o la eliminación de los mismos. Nótese que la eliminación no es la mejor alternativa en la mayoría de casos, debido a que de esta manera se deja de utilizar información valiosa para la tarea de clasificación. Por tanto, en este caso se ha decidido realizar una discretización.

		\paragraph{}
		El método que se ha utilizado para dicha tarea ha sido el más básico posible, realizando una partición en \textbf{4 intervalos de la misma anchura} sobre el rango de valores que toma cada uno de los atributos numéricos. Tras procesar el conjunto de datos con el algoritmo \emph{ID3} teniendo en cuenta la metodología experimental citada anteriormente, el árbol de decisión generado se ilustra en la figura \ref{fig:id3-tree}. Tal y como se puede apreciar, este árbol presenta muchas ramificaciones. La causa de ello es que por cada atributo numérico, se han generado 4 ramas y dado que \emph{ID3} genera todas las posibles combinaciones posibles a partir de las instancias destinadas a la fase de entrenamiento, el fenómeno combinatorio produce dicho efecto.

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=\textwidth]{id3-tree}
			\end{center}
			\caption{Árbol de decisión generado a partir del algoritmo \emph{ID3}}
			\label{fig:id3-tree}
		\end{figure}

		\paragraph{}
		La matriz de confusión resultante de dicha clasificación se muestra en la tabla \ref{table:confusion-matrix-id3}. Nótese que se han utilizado \textbf{160 instancias} pero el árbol de decisión ha dejado sin clasificar $5$ de ellas, por lo que la tasa de acierto global es del $72.5\%$.

		\begin{table}[h]
			\begin{center}
				\begin{tabular}{r c c|c|l}
					& & \multicolumn{2}{ c }{Valor Real} \\ \cline{3-4}
					& & \multicolumn{1}{ |c| }{Positivo} & \multicolumn{1}{ |c| }{Negativo} & \multicolumn{1}{ l }{$p_i$}\\ \cline{2-4}
					\multicolumn{1}{  c  }{\multirow{2}{*}{Valor Predicho} } 	& \multicolumn{1}{ |c| }{Positivo} & $5$ & $18$ &  $0.2173$   \\ \cline{2-4}
					\multicolumn{1}{  c  }{}                        					& \multicolumn{1}{ |c| }{Negativo} & $21$  & $111$ & $0.8409$ \\ \cline{2-4}
					& \multicolumn{1}{ c }{$\pi_j$} & \multicolumn{1}{ c }{$0.1677$} & \multicolumn{1}{ c }{$0.8322$} & \multicolumn{1}{ l }{$N = 155$}
				\end{tabular}
			\end{center}
			\caption{Matriz de confusión del conjunto de datos discretizado previamente y después entrenado por el algoritmo \emph{ID3}}
			\label{table:confusion-matrix-id3}
		\end{table}


	\section{Volviendo al fichero original, pase el algoritmo J48 también aplicando el método de retención o Hold-Out. Analice el árbol obtenido sin poda. ¿Se podría prescindir de algún atributo? Si es así, hágalo y compare los resultados de nuevo}
	\label{sec:3}

		\paragraph{}
		Puesto que el algoritmo \emph{J48} si que permite el procesado de conjunto de datos con atributos continuos, en este caso no es necesario realizar ninguna tarea previa para la generación del árbol de decisión. Utilizando la misma metodología de la sección anterior (Hold-Out de $1/3$) se obtiene el árbol de decisión de la figura \ref{fig:j48-tree-all}

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=\textwidth]{j48-tree-all}
			\end{center}
			\caption{Árbol de decisión generado a partir del algoritmo \emph{J48}}
			\label{fig:j48-tree-all}
		\end{figure}

		\paragraph{}
		Tras analizar el árbol se ha comprobado que $2$ de los atributos (\textbf{PRE19} y \textbf{PRE32}) no se han escogido para la construcción del árbol de decisión, por tanto, posteriormente se ha vuelto a generar a partir del algoritmo \emph{J48} otro árbol de decisión, pero esta vez eliminando del conjunto de datos dichos atributos. La matriz de confusión obtenida en con todos los atributos se muestra en la tabla \ref{table:confusion-matrix-j48-all}. La tasa de acierto resultante de dicha clasificación ha sido del $75\%$.

		\begin{table}[h]
			\begin{center}
				\begin{tabular}{r c c|c|l}
					& & \multicolumn{2}{ c }{Valor Real} \\ \cline{3-4}
					& & \multicolumn{1}{ |c| }{Positivo} & \multicolumn{1}{ |c| }{Negativo} & \multicolumn{1}{ l }{$p_i$}\\ \cline{2-4}
					\multicolumn{1}{  c  }{\multirow{2}{*}{Valor Predicho} } 	& \multicolumn{1}{ |c| }{Positivo} & $4$ & $18$ &  $0.1375$   \\ \cline{2-4}
					\multicolumn{1}{  c  }{}                        					& \multicolumn{1}{ |c| }{Negativo} & $22$  & $116$ & $0.8625$ \\ \cline{2-4}
					& \multicolumn{1}{ c }{$\pi_j$} & \multicolumn{1}{ c }{$0.1625$} & \multicolumn{1}{ c }{$0.8375$} & \multicolumn{1}{ l }{$N = 160$}
				\end{tabular}
			\end{center}
			\caption{Matriz de confusión del conjunto de datos entrenado por el algoritmo \emph{J48}}
			\label{table:confusion-matrix-j48-all}
		\end{table}

		\paragraph{}
		El árbol resultante de aplicar el algoritmo \emph{J48} eliminando los atributos \textbf{PRE19} y \textbf{PRE32} se muestra en la figura \ref{fig:j48-tree-some}. Además, en la tabla \ref{table:confusion-matrix-j48-all} se muestra la matriz de confusión, cuyos resultados son equivalentes al caso anterior. A pesar de ello el árbol de decisión presenta una estructura diferente.


		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=\textwidth]{j48-tree-some}
			\end{center}
			\caption{Árbol de decisión generado a partir del algoritmo \emph{J48} tras eliminar los atributos no utilizados previamente.}
			\label{fig:j48-tree-some}
		\end{figure}

		\paragraph{}
		La razón por la cual sucede esto es debido a que la implementación del algoritmo \emph{J48} implementada en \emph{Weka}\cite{tool:weka} utiliza una medida ligeramente diferente de \textbf{Ganancia de Información} con respecto de la básica. La principal diferencia subyace en que la ganancia basada en ratio divide el resultado de la Ganancia en Información entre la entropía del atributo, lo cual produce una variación de resultados dependiendo del número de atributos que tenga el conjunto de datos. Esto se ilustra de forma más clara en la ecuación \eqref{eq:gain_ratio}

		\begin{equation}
		\label{eq:gain_ratio}
		GainRatio(Class, Attribute) = \frac{Gain(Class, Attribute)}{H(Attribute)} = \frac{H(Class) - H(Class | Attribute)}{H(Attribute)}
		\end{equation}


		\begin{table}[h]
			\begin{center}
				\begin{tabular}{r c c|c|l}
					& & \multicolumn{2}{ c }{Valor Real} \\ \cline{3-4}
					& & \multicolumn{1}{ |c| }{Positivo} & \multicolumn{1}{ |c| }{Negativo} & \multicolumn{1}{ l }{$p_i$}\\ \cline{2-4}
					\multicolumn{1}{  c  }{\multirow{2}{*}{Valor Predicho} } 	& \multicolumn{1}{ |c| }{Positivo} & $4$ & $18$ &  $0.1375$   \\ \cline{2-4}
					\multicolumn{1}{  c  }{}                        					& \multicolumn{1}{ |c| }{Negativo} & $22$  & $116$ & $0.8625$ \\ \cline{2-4}
					& \multicolumn{1}{ c }{$\pi_j$} & \multicolumn{1}{ c }{$0.1625$} & \multicolumn{1}{ c }{$0.8375$} & \multicolumn{1}{ l }{$N = 160$}
				\end{tabular}
			\end{center}
			\caption{Matriz de confusión del conjunto de datos entrenado por el algoritmo \emph{J48}tras eliminar los atributos no utilizados previamente.}
			\label{table:confusion-matrix-j48-all}
		\end{table}

	\section{Habrá notado que cuando se usa algún atributo numérico, implícitamente se aplica una discretización al plantear las diferentes ramas del árbol a partir de él. ¿Por qué es más eficiente esta técnica que la aplicada en \ref{sec:2}?}
	\label{sec:4}

		\paragraph{}
		La técnica seguida por el algoritmo \emph{J48} ofrece mejores resultados que la aplicada en el apartado \ref{sec:2} con el algoritmo \emph{ID3} debido a que en dicho caso la discretización se realiza \emph{A priori}. Esto quiere decir que además de no poder apoyarse en los valores de la clase de destino para la clasificación, este atributo tan solo podrá ser utilizado una vez para la clasificación.

		\paragraph{}
 		Por contra, el algoritmo \emph{J48} realiza una técnica que aplica particionamiento binario y recursivo utilizando la \emph{Ganancia de Información} como medida de bondad. La técnica de particionamiento recursivo de atributos numéricos permite una clasificación más eficiente ya que tan solo se produce cuando se maximiza la ganancia de información para dicha partición binaria.

	\section{Plantee, entonces, una discretización basada en el punto anterior, aunque no resulte ser binaria, sino en tantos tramos como induzcan los valores usados al formar las ramas del árbol con J48 sin poda (Hold-Out). Introduzca este fichero de nuevo al algoritmo ID3. Compare los resultados con el J48 de \ref{sec:3}}
	\label{sec:5}

		\paragraph{}
		Para realizar la discretización propuesta, se ha realizado un script básico en python, el cual se puede visualizar a partir de \url{https://github.com/garciparedes/python-examples/blob/master/data_sets/main/DiscretizeThoraricSurgery.py}\cite{github:garciparedes-python-examples}. Tal y como se produce en el caso del árbol de decisión generado por \emph{J48} en el apartado \ref{sec:2}, se ha dividido el atributo \textbf{PRE4} en 3 particiones, el atributo \textbf{PRE5} en 2 particiones y el atributo \textbf{AGE} en 4 fijando los límites de cada uno de ellos según los mismos puntos que hizo \emph{J48}. El árbol resultante generado por \emph{ID3} se muestra en la figura \ref{fig:id3-tree-2}. Nótese que el tamaño de este es mucho mayor que su homónimo generado por \emph{J48}.

		\begin{figure}[h]
			\begin{center}
				\includegraphics[width=\textwidth]{id3-tree-2}
			\end{center}
			\caption{Árbol de decisión generado a partir del algoritmo \emph{ID3} discretizado según las particiones del apartado \ref{sec:3}}
			\label{fig:id3-tree-2}
		\end{figure}

		\paragraph{}
		La matriz de confusión resultante de dicha clasificación se muestra en la tabla \ref{table:confusion-matrix-id3-2}. Nótese que se han utilizado \textbf{160 instancias} pero el árbol de decisión ha dejado sin clasificar $2$ de ellas, por lo que la tasa de acierto global es del $77.5\%$.

		\begin{table}[h]
			\begin{center}
				\begin{tabular}{r c c|c|l}
					& & \multicolumn{2}{ c }{Valor Real} \\ \cline{3-4}
					& & \multicolumn{1}{ |c| }{Positivo} & \multicolumn{1}{ |c| }{Negativo} & \multicolumn{1}{ l }{$p_i$}\\ \cline{2-4}
					\multicolumn{1}{  c  }{\multirow{2}{*}{Valor Predicho} } 	& \multicolumn{1}{ |c| }{Positivo} & $4$ & $13$ &  $0.1075$   \\ \cline{2-4}
					\multicolumn{1}{  c  }{}                        					& \multicolumn{1}{ |c| }{Negativo} & $21$  & $120$ & $0.8924$ \\ \cline{2-4}
					& \multicolumn{1}{ c }{$\pi_j$} & \multicolumn{1}{ c }{$0.1582$} & \multicolumn{1}{ c }{$0.8417$} & \multicolumn{1}{ l }{$N = 158$}
				\end{tabular}
			\end{center}
			\caption{Matriz de confusión del conjunto de datos discretizado a partir de la división de \ref{sec:2} y después entrenado por el algoritmo \emph{ID3}}
			\label{table:confusion-matrix-id3-2}
		\end{table}

		\paragraph{}
		El árbol obtenido en este apartado ha presentado mejores resultados durante la fase de test. Los motivos por los que ha ocurrido dicho fenómeno pueden haber sido debidos a lo siguiente:

		\begin{itemize}
			\item Los intervalos de particionamiento de los atributos de naturaleza numérica para su discretización han sido seleccionados de forma óptima según los resulados anteriores del algoritmo \emph{J48}, que trata de maximizar la Ganancia de Información al realizar dicha tarea.
			\item El árbol captura todas las posibles combinaciones en cada caso dado que la discretización se ha realizado \emph{A priori}. Esto dependiendo del conjunto de datos, puede ser ser una ventaja o una desventaja debido a que en un gran número de ocasiones el sobreajuste que produce \emph{ID3} es perjudicial para la tarea de clasificación, aunque en este caso no ha sido así.
		\end{itemize}

%-----------------------------
%	Bibliographic references
%-----------------------------
	\nocite{subject:taa}
	\nocite{github:ismtabo-treetograph}
  \bibliographystyle{acm}
  \bibliography{bib/misc}

\end{document}
